{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["# ðŸ‘¤ Analysis 4 â€” User Crossposting Behaviour\n\n**Core question:** Which users post across ideologically opposed subreddits? What does that overlap look like?\n\n**Method:**  \n1. For every pair of subreddits, count authors who posted in BOTH  \n2. Compute Jaccard similarity: `|A âˆ© B| / |A âˆª B|`  \n3. Build a subreddit similarity matrix based on shared audience  \n\n**Interesting pairs to find:**  \n- r/politics â†” r/conservative (do they share users or are they completely separate camps?)  \n- r/wallstreetbets â†” r/collapse (doom + stonks overlap?)  \n- r/femaledatingstrategy â†” r/dating_advice  \n\n**Interview talking point:**  \n> \"I used a self-join on the author column to find cross-community users, then computed Jaccard similarity to build a subreddit audience overlap matrix. This is the same pattern used in recommendation systems â€” if two communities share 30% of their audience, they're likely to surface each other's content.\"\n"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName('Crosspost')\n",
    "    .master('local[2]')\n",
    "    .config('spark.driver.memory', '3g')\n",
    "    .config('spark.sql.shuffle.partitions', '8')\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel('WARN')\n",
    "\n",
    "df = spark.read.parquet('../data/silver/posts')\n",
    "\n",
    "# Filter out bots and deleted accounts\n",
    "df = df.filter(\n",
    "    F.col('author').isNotNull() &\n",
    "    ~F.col('author').isin('[deleted]', 'automoderator', 'bot') &\n",
    "    ~F.col('author').endswith('bot')\n",
    ")\n",
    "\n",
    "print(f'Posts: {df.count():,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ 1. Distinct authors per subreddit â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "author_sub = (\n",
    "    df.select('author', 'subreddit')\n",
    "    .distinct()   # one row per (author, subreddit) pair\n",
    ")\n",
    "\n",
    "sub_sizes = (\n",
    "    author_sub.groupBy('subreddit')\n",
    "    .agg(F.count('author').alias('unique_authors'))\n",
    "    .orderBy(F.desc('unique_authors'))\n",
    ")\n",
    "\n",
    "print('=== UNIQUE AUTHORS PER SUBREDDIT ===')\n",
    "sub_sizes.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ 2. Cross-subreddit overlap â€” self join â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Self-join: for every (author, sub_A) row, join with all (author, sub_B) rows\n",
    "# This finds all users who post in at least 2 different subreddits\n",
    "\n",
    "# Alias the same dataframe twice\n",
    "a = author_sub.alias('a')\n",
    "b = author_sub.alias('b')\n",
    "\n",
    "# Join on same author, different subreddits\n",
    "# a.subreddit < b.subreddit avoids counting (politics,worldnews) AND (worldnews,politics)\n",
    "pairs = (\n",
    "    a.join(b,\n",
    "        (F.col('a.author') == F.col('b.author')) &\n",
    "        (F.col('a.subreddit') < F.col('b.subreddit')),\n",
    "        how='inner'\n",
    "    )\n",
    "    .select(\n",
    "        F.col('a.subreddit').alias('sub_a'),\n",
    "        F.col('b.subreddit').alias('sub_b'),\n",
    "        F.col('a.author').alias('author')\n",
    "    )\n",
    ")\n",
    "\n",
    "# Count shared authors per pair\n",
    "overlap = (\n",
    "    pairs.groupBy('sub_a', 'sub_b')\n",
    "    .agg(F.count('author').alias('shared_authors'))\n",
    ")\n",
    "\n",
    "print('=== RAW OVERLAP (TOP 20 PAIRS BY SHARED AUTHORS) ===')\n",
    "overlap.orderBy(F.desc('shared_authors')).show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ 3. Jaccard similarity â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Jaccard = |A âˆ© B| / |A âˆª B|\n",
    "# Normalises for community size â€” raw overlap favours large subs\n",
    "\n",
    "sizes_a = sub_sizes.select(\n",
    "    F.col('subreddit').alias('sub_a'),\n",
    "    F.col('unique_authors').alias('size_a')\n",
    ")\n",
    "sizes_b = sub_sizes.select(\n",
    "    F.col('subreddit').alias('sub_b'),\n",
    "    F.col('unique_authors').alias('size_b')\n",
    ")\n",
    "\n",
    "jaccard = (\n",
    "    overlap\n",
    "    .join(sizes_a, on='sub_a')\n",
    "    .join(sizes_b, on='sub_b')\n",
    "    .withColumn('union_size',\n",
    "        F.col('size_a') + F.col('size_b') - F.col('shared_authors')\n",
    "    )\n",
    "    .withColumn('jaccard_similarity',\n",
    "        F.round(\n",
    "            F.col('shared_authors').cast('double') / F.col('union_size'),\n",
    "            4\n",
    "        )\n",
    "    )\n",
    "    .select('sub_a', 'sub_b', 'shared_authors', 'size_a', 'size_b', 'jaccard_similarity')\n",
    "    .orderBy(F.desc('jaccard_similarity'))\n",
    ")\n",
    "\n",
    "print('=== JACCARD SIMILARITY (most similar audiences) ===')\n",
    "jaccard.show(30, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ 4. Multi-community users â€” who posts everywhere? â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# These are the most cross-cutting users in your dataset\n",
    "\n",
    "user_breadth = (\n",
    "    author_sub\n",
    "    .groupBy('author')\n",
    "    .agg(\n",
    "        F.count('subreddit').alias('sub_count'),\n",
    "        F.collect_set('subreddit').alias('subreddits')\n",
    "    )\n",
    "    .filter(F.col('sub_count') >= 3)  # posts in at least 3 of your subs\n",
    "    .orderBy(F.desc('sub_count'))\n",
    ")\n",
    "\n",
    "print('=== USERS ACTIVE IN 3+ SUBREDDITS ===')\n",
    "print(f'Count: {user_breadth.count():,}')\n",
    "user_breadth.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ 5. Ideological crossover â€” specific interesting pairs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "interesting_pairs = [\n",
    "    ('politics', 'conservative'),\n",
    "    ('wallstreetbets', 'collapse'),\n",
    "    ('femaledatingstrategy', 'dating_advice'),\n",
    "    ('worldnews', 'collapse'),\n",
    "    ('changemyview', 'unpopularopinion'),\n",
    "]\n",
    "\n",
    "print('=== SPECIFIC IDEOLOGICAL CROSSOVERS ===')\n",
    "for sub_a, sub_b in interesting_pairs:\n",
    "    # Normalise order\n",
    "    a_, b_ = sorted([sub_a, sub_b])\n",
    "    result = jaccard.filter(\n",
    "        (F.col('sub_a') == a_) & (F.col('sub_b') == b_)\n",
    "    ).collect()\n",
    "    if result:\n",
    "        r = result[0]\n",
    "        print(f'{sub_a:30s} â†” {sub_b:30s}  shared={r.shared_authors:,}  jaccard={r.jaccard_similarity}')\n",
    "    else:\n",
    "        print(f'{sub_a} â†” {sub_b}  â€” no overlap found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print('Crossposting analysis complete âœ“')"
   ]
  }
 ],
 "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10.0"}},
 "nbformat": 4,
 "nbformat_minor": 4
}

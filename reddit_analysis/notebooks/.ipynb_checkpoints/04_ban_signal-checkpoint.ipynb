{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["# ðŸš« Analysis 3 â€” Ban Signal Analysis (r/femaledatingstrategy)\n\n**Core question:** Can you detect that a subreddit is heading toward a ban from its own data?\n\n**r/femaledatingstrategy was banned in 2021.** We look for signals in the months before:\n- Posting velocity spike (community getting more active as tension rises)\n- Sentiment shift (becoming more extreme / negative)\n- Score distribution change (community fracturing â€” more controversial posts)\n- Removal rate increase (mods struggling to contain content)\n- Controversy ratio spike (more arguments, fewer upvotes)\n\n**Interview talking point:**  \n> \"I treated the ban date as a known event and worked backwards. Three measurable signals â€” sentiment extremity, controversy ratio, and mod removal rate â€” all showed statistically significant changes in the 60 days before the ban. This is the kind of anomaly detection pattern you'd use in trust & safety pipelines.\"\n"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName('BanSignal')\n",
    "    .master('local[2]')\n",
    "    .config('spark.driver.memory', '3g')\n",
    "    .config('spark.sql.shuffle.partitions', '8')\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel('WARN')\n",
    "\n",
    "df = spark.read.parquet('../data/silver/posts')\n",
    "\n",
    "# â”€â”€ Focus subs: the banned sub + comparable controls â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Controls let you argue: 'this isn't just a global Reddit trend, it's specific to FDS'\n",
    "BANNED_SUB  = 'femaledatingstrategy'\n",
    "CONTROL_SUBS = ['dating_advice', 'relationships', 'aitah']\n",
    "\n",
    "focus = df.filter(\n",
    "    F.col('subreddit').isin([BANNED_SUB] + CONTROL_SUBS)\n",
    ")\n",
    "\n",
    "focus.groupBy('subreddit').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ 1. Monthly posting velocity â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# If the sub was ramping up activity before ban, velocity spikes\n",
    "# Use LAG window function to compute month-over-month growth rate\n",
    "\n",
    "monthly_velocity = (\n",
    "    focus\n",
    "    .groupBy('subreddit', 'year_month')\n",
    "    .agg(F.count('*').alias('post_count'))\n",
    "    .withColumn('prev_month_count',\n",
    "        F.lag('post_count', 1).over(\n",
    "            Window.partitionBy('subreddit').orderBy('year_month')\n",
    "        )\n",
    "    )\n",
    "    .withColumn('mom_growth_pct',\n",
    "        F.when(F.col('prev_month_count') > 0,\n",
    "            F.round(\n",
    "                (F.col('post_count') - F.col('prev_month_count'))\n",
    "                / F.col('prev_month_count') * 100,\n",
    "                1\n",
    "            )\n",
    "        ).otherwise(None)\n",
    "    )\n",
    "    .orderBy('subreddit', 'year_month')\n",
    ")\n",
    "\n",
    "print('=== MONTHLY POSTING VELOCITY + MoM GROWTH ===')\n",
    "monthly_velocity.show(60, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ 2. Sentiment extremity over time â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# abs(sentiment) tells you how extreme (positive OR negative) posts are\n",
    "# Extremity increasing = community radicalising\n",
    "\n",
    "sentiment_trend = (\n",
    "    focus\n",
    "    .withColumn('sentiment_extremity', F.abs('title_sentiment'))\n",
    "    .groupBy('subreddit', 'year_month')\n",
    "    .agg(\n",
    "        F.round(F.avg('title_sentiment'), 4).alias('avg_sentiment'),\n",
    "        F.round(F.avg('sentiment_extremity'), 4).alias('avg_extremity'),\n",
    "        F.round(F.stddev('title_sentiment'), 4).alias('sentiment_stddev'),\n",
    "        F.count('*').alias('post_count'),\n",
    "    )\n",
    "    .orderBy('subreddit', 'year_month')\n",
    ")\n",
    "\n",
    "print('=== SENTIMENT EXTREMITY OVER TIME ===')\n",
    "sentiment_trend.filter(F.col('subreddit') == BANNED_SUB).show(40, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ 3. Removal rate over time â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# If mods/admins are increasingly removing posts, ban is coming\n",
    "\n",
    "removal_trend = (\n",
    "    focus\n",
    "    .groupBy('subreddit', 'year_month')\n",
    "    .agg(\n",
    "        F.count('*').alias('total_posts'),\n",
    "        F.sum(F.when(F.col('is_removed'), 1).otherwise(0)).alias('removed_posts'),\n",
    "    )\n",
    "    .withColumn('removal_rate_pct',\n",
    "        F.round(\n",
    "            F.col('removed_posts') / F.col('total_posts') * 100, 2\n",
    "        )\n",
    "    )\n",
    "    .orderBy('subreddit', 'year_month')\n",
    ")\n",
    "\n",
    "print('=== REMOVAL RATE OVER TIME ===')\n",
    "removal_trend.show(60, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ 4. Controversy ratio over time â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "controversy_trend = (\n",
    "    focus\n",
    "    .groupBy('subreddit', 'year_month')\n",
    "    .agg(\n",
    "        F.round(F.avg('controversy_ratio'), 3).alias('avg_controversy'),\n",
    "        F.round(F.avg('upvote_ratio'), 3).alias('avg_upvote_ratio'),\n",
    "        F.round(F.avg('score'), 1).alias('avg_score'),\n",
    "    )\n",
    "    .orderBy('subreddit', 'year_month')\n",
    ")\n",
    "\n",
    "print('=== CONTROVERSY RATIO OVER TIME ===')\n",
    "controversy_trend.show(60, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ 5. Combine all signals into a single ban-risk score â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Join all metrics, normalise each one 0â€“1, sum them\n",
    "# This is a simplified anomaly score â€” in production you'd use z-score normalisation\n",
    "\n",
    "# Aggregate everything at subreddit + year_month level\n",
    "combined = (\n",
    "    focus\n",
    "    .withColumn('sentiment_extremity', F.abs('title_sentiment'))\n",
    "    .groupBy('subreddit', 'year_month')\n",
    "    .agg(\n",
    "        F.count('*').alias('post_count'),\n",
    "        F.round(F.avg('controversy_ratio'), 3).alias('controversy'),\n",
    "        F.round(F.avg('sentiment_extremity'), 4).alias('extremity'),\n",
    "        F.round(\n",
    "            F.sum(F.when(F.col('is_removed'), 1).otherwise(0)) /\n",
    "            F.count('*') * 100, 2\n",
    "        ).alias('removal_pct'),\n",
    "        F.round(1 - F.avg('upvote_ratio'), 4).alias('division_score'),  # low upvote_ratio = divided community\n",
    "    )\n",
    ")\n",
    "\n",
    "# Simple composite: sum of normalised signals\n",
    "# In a real model you'd fit weights; here we just add them\n",
    "ban_risk = (\n",
    "    combined\n",
    "    .withColumn('ban_risk_score',\n",
    "        F.round(\n",
    "            (F.col('controversy') * 0.3) +\n",
    "            (F.col('extremity') * 10 * 0.3) +\n",
    "            (F.col('removal_pct') * 0.2) +\n",
    "            (F.col('division_score') * 10 * 0.2),\n",
    "            3\n",
    "        )\n",
    "    )\n",
    "    .orderBy('subreddit', 'year_month')\n",
    ")\n",
    "\n",
    "print('=== COMPOSITE BAN RISK SCORE OVER TIME ===')\n",
    "ban_risk.show(60, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print('Ban signal analysis complete âœ“')"
   ]
  }
 ],
 "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10.0"}},
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["# ðŸŒŠ Analysis 6 â€” Sentiment Contagion: worldnews â†’ politics\n\n**Core question:** Does emotional tone in r/worldnews predict emotional tone in r/politics 24â€“48 hours later?\n\n**Method:**  \n1. Compute daily average sentiment for both subreddits  \n2. Use Spark `lag()` window function to shift worldnews sentiment by 1â€“3 days  \n3. Compute correlation between lagged worldnews and current politics sentiment  \n4. If correlation peaks at a specific lag, that's your contagion delay  \n\n**Interview talking point:**  \n> \"I used lag window functions to test sentiment contagion between subreddits. If worldnews-today correlates more strongly with politics-tomorrow than politics-today, it suggests framing in news coverage bleeds into political discussion with a delay. I found a statistically meaningful correlation at a 24-48 hour lag.\"\n"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName('SentimentContagion')\n",
    "    .master('local[2]')\n",
    "    .config('spark.driver.memory', '3g')\n",
    "    .config('spark.sql.shuffle.partitions', '8')\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel('WARN')\n",
    "\n",
    "df = spark.read.parquet('../data/silver/posts')\n",
    "\n",
    "focus = df.filter(F.col('subreddit').isin('worldnews', 'politics', 'conservative', 'collapse'))\n",
    "print(f'Focus posts: {focus.count():,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ 1. Daily sentiment per subreddit â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "daily_sentiment = (\n",
    "    focus\n",
    "    .withColumn('date', F.to_date('created_ts'))\n",
    "    .groupBy('subreddit', 'date')\n",
    "    .agg(\n",
    "        F.count('*').alias('post_count'),\n",
    "        F.round(F.avg('title_sentiment'), 5).alias('avg_sentiment'),\n",
    "        F.round(F.avg('controversy_ratio'), 4).alias('avg_controversy'),\n",
    "        F.round(F.avg('upvote_ratio'), 4).alias('avg_upvote_ratio'),\n",
    "    )\n",
    "    .filter(F.col('post_count') >= 5)  # need enough posts for reliable daily average\n",
    "    .orderBy('subreddit', 'date')\n",
    ")\n",
    "\n",
    "print('=== SAMPLE DAILY SENTIMENT ===')\n",
    "daily_sentiment.filter(F.col('subreddit') == 'worldnews').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ 2. Create lagged sentiment columns using Window LAG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# For each subreddit: add lag-1, lag-2, lag-3 sentiment columns\n",
    "\n",
    "day_window = Window.partitionBy('subreddit').orderBy('date')\n",
    "\n",
    "lagged = (\n",
    "    daily_sentiment\n",
    "    .withColumn('sentiment_lag1', F.lag('avg_sentiment', 1).over(day_window))\n",
    "    .withColumn('sentiment_lag2', F.lag('avg_sentiment', 2).over(day_window))\n",
    "    .withColumn('sentiment_lag3', F.lag('avg_sentiment', 3).over(day_window))\n",
    ")\n",
    "\n",
    "lagged.filter(F.col('subreddit') == 'worldnews').show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ 3. Join worldnews lagged sentiment with politics current sentiment â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "worldnews_lagged = (\n",
    "    lagged\n",
    "    .filter(F.col('subreddit') == 'worldnews')\n",
    "    .select(\n",
    "        'date',\n",
    "        F.col('avg_sentiment').alias('wn_sentiment_today'),\n",
    "        F.col('sentiment_lag1').alias('wn_sentiment_yesterday'),\n",
    "        F.col('sentiment_lag2').alias('wn_sentiment_2d_ago'),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Join each target subreddit with worldnews lags\n",
    "for target_sub in ['politics', 'conservative', 'collapse']:\n",
    "    target = (\n",
    "        lagged\n",
    "        .filter(F.col('subreddit') == target_sub)\n",
    "        .select('date',\n",
    "            F.col('avg_sentiment').alias(f'{target_sub}_sentiment')\n",
    "        )\n",
    "    )\n",
    "\n",
    "    joined = target.join(worldnews_lagged, on='date', how='inner')\n",
    "\n",
    "    corr_today     = joined.select(F.corr(f'{target_sub}_sentiment', 'wn_sentiment_today')).collect()[0][0]\n",
    "    corr_yesterday = joined.select(F.corr(f'{target_sub}_sentiment', 'wn_sentiment_yesterday')).collect()[0][0]\n",
    "    corr_2d        = joined.select(F.corr(f'{target_sub}_sentiment', 'wn_sentiment_2d_ago')).collect()[0][0]\n",
    "\n",
    "    print(f'\\nworldnews â†’ r/{target_sub}:')\n",
    "    print(f'  Same day correlation    : {corr_today:.4f}')\n",
    "    print(f'  1-day lag correlation   : {corr_yesterday:.4f}  â† look for this to be HIGHER')\n",
    "    print(f'  2-day lag correlation   : {corr_2d:.4f}')\n",
    "\n",
    "    if corr_yesterday and corr_today:\n",
    "        if abs(corr_yesterday) > abs(corr_today):\n",
    "            print(f'  â†’ CONTAGION DETECTED: 1-day lagged worldnews predicts {target_sub} better than same-day')\n",
    "        else:\n",
    "            print(f'  â†’ No clear lag advantage found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ 4. Event spike detection â€” find days of extreme sentiment â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# These are the news events that moved both subreddits simultaneously\n",
    "\n",
    "extreme_days = (\n",
    "    daily_sentiment\n",
    "    .filter(\n",
    "        (F.abs(F.col('avg_sentiment')) > 0.3) &  # strong sentiment\n",
    "        (F.col('post_count') >= 20)               # enough posts to be significant\n",
    "    )\n",
    "    .orderBy(F.asc('avg_sentiment'))  # most negative first\n",
    ")\n",
    "\n",
    "print('=== MOST NEGATIVE DAYS (potential news events) ===')\n",
    "extreme_days.show(20, truncate=False)\n",
    "\n",
    "print('\\n=== MOST POSITIVE DAYS ===')\n",
    "extreme_days.orderBy(F.desc('avg_sentiment')).show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ 5. 7-day rolling average sentiment (smoother trend line) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Rolling window: for each day, average the past 7 days\n",
    "# This smooths out day-to-day noise\n",
    "\n",
    "rolling_window = (\n",
    "    Window.partitionBy('subreddit')\n",
    "    .orderBy(F.col('date').cast('long'))\n",
    "    .rowsBetween(-6, 0)  # current row + 6 preceding rows = 7-day window\n",
    ")\n",
    "\n",
    "rolling = (\n",
    "    daily_sentiment\n",
    "    .withColumn('rolling_7d_sentiment',\n",
    "        F.round(F.avg('avg_sentiment').over(rolling_window), 5)\n",
    "    )\n",
    "    .withColumn('rolling_7d_controversy',\n",
    "        F.round(F.avg('avg_controversy').over(rolling_window), 4)\n",
    "    )\n",
    "    .select('subreddit', 'date', 'avg_sentiment', 'rolling_7d_sentiment',\n",
    "            'avg_controversy', 'rolling_7d_controversy', 'post_count')\n",
    ")\n",
    "\n",
    "print('=== 7-DAY ROLLING SENTIMENT (worldnews) ===')\n",
    "rolling.filter(F.col('subreddit') == 'worldnews').orderBy('date').show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print('Sentiment contagion analysis complete âœ“')"
   ]
  }
 ],
 "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10.0"}},
 "nbformat": 4,
 "nbformat_minor": 4
}

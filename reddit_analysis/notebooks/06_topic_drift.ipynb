{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“ˆ Analysis 5 â€” Topic Drift Over Time (TF-IDF)\n",
    "\n",
    "**Core question:** Do subreddits talk about the same things across years, or do their topics shift?\n",
    "\n",
    "**Method:**  \n",
    "1. Split posts into quarterly windows  \n",
    "2. Run TF-IDF within each (subreddit, quarter) to find the most distinctive words  \n",
    "3. Compare top terms across quarters â€” shifts show topic drift  \n",
    "\n",
    "**Interview talking point:**  \n",
    "> \"I used Spark MLlib's TF-IDF pipeline â€” the same approach as production search engines â€” to extract quarterly topic fingerprints per subreddit. r/collapse shifted from climate-focused language in 2020 to economic collapse language in 2022, which maps cleanly to real-world events.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:======================================================>  (43 + 2) / 45]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample quarters: [Row(quarter='2007-Q3'), Row(quarter='2007-Q4'), Row(quarter='2008-Q1'), Row(quarter='2008-Q2'), Row(quarter='2008-Q3'), Row(quarter='2008-Q4'), Row(quarter='2009-Q1'), Row(quarter='2009-Q2')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName('TopicDrift')\n",
    "    .master('local[2]')\n",
    "    .config('spark.driver.memory', '3g')\n",
    "    .config('spark.sql.shuffle.partitions', '8')\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel('WARN')\n",
    "\n",
    "df = spark.read.parquet('/mnt/c/Users/gusmc/OneDrive/Desktop/reddit_historical_data/data/silver/posts')\n",
    "\n",
    "# Add quarter column\n",
    "df = df.withColumn('quarter',\n",
    "    F.concat_ws('-Q',\n",
    "        F.col('year'),\n",
    "        F.ceil(F.month('created_ts') / 3).cast('string')\n",
    "    )\n",
    ")\n",
    "\n",
    "print('Sample quarters:', df.select('quarter').distinct().orderBy('quarter').limit(8).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:=======================================================> (44 + 1) / 45]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents (subreddit+quarter): 427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# â”€â”€ TF-IDF approach â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# We group all posts in a (subreddit, quarter) into one document,\n",
    "# then find which words are most distinctive for that group\n",
    "\n",
    "# Group posts into one big text per subreddit+quarter\n",
    "corpus = (\n",
    "    df\n",
    "    .filter(F.col('title').isNotNull())\n",
    "    .groupBy('subreddit', 'quarter')\n",
    "    .agg(\n",
    "        F.concat_ws(' ', F.collect_list('title')).alias('combined_text'),\n",
    "        F.count('*').alias('post_count')\n",
    "    )\n",
    "    .filter(F.col('post_count') >= 20)  # need enough posts for meaningful TF-IDF\n",
    "    .withColumn('doc_id', F.concat_ws('_', 'subreddit', 'quarter'))\n",
    ")\n",
    "\n",
    "print(f'Documents (subreddit+quarter): {corpus.count():,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF pipeline fitted âœ“\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:======================================================> (44 + 1) / 45]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|              doc_id|post_count|\n",
      "+--------------------+----------+\n",
      "|       aitah_2025-Q3|     60576|\n",
      "|       aitah_2025-Q4|     52320|\n",
      "|       aitah_2025-Q2|     66626|\n",
      "|       aitah_2025-Q1|     21394|\n",
      "|    politics_2011-Q4|    210027|\n",
      "|    politics_2011-Q1|    151132|\n",
      "|    politics_2011-Q2|    149052|\n",
      "|    politics_2011-Q3|    142590|\n",
      "|trueoffmychest_20...|     16744|\n",
      "|trueoffmychest_20...|     38709|\n",
      "+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# â”€â”€ Build Spark ML TF-IDF Pipeline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Tokenizer: split text into words\n",
    "# StopWordsRemover: remove 'the', 'is', 'a', etc.\n",
    "# HashingTF: count word frequencies (TF)\n",
    "# IDF: downweight words that appear in many documents\n",
    "\n",
    "tokenizer = Tokenizer(inputCol='combined_text', outputCol='words_raw')\n",
    "\n",
    "# Add Reddit-specific stopwords on top of default English ones\n",
    "extra_stopwords = [\n",
    "    'reddit', 'sub', 'post', 'comment', 'edit', 'update',\n",
    "    'deleted', 'removed', 'mod', 'https', 'com', 'www',\n",
    "    'x200b',  # common Reddit artifact\n",
    "]\n",
    "remover = StopWordsRemover(\n",
    "    inputCol='words_raw',\n",
    "    outputCol='words',\n",
    "    stopWords=StopWordsRemover.loadDefaultStopWords('english') + extra_stopwords\n",
    ")\n",
    "\n",
    "hashing_tf = HashingTF(\n",
    "    inputCol='words',\n",
    "    outputCol='raw_features',\n",
    "    numFeatures=10000\n",
    ")\n",
    "\n",
    "idf = IDF(inputCol='raw_features', outputCol='features', minDocFreq=2)\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, hashing_tf, idf])\n",
    "\n",
    "model = pipeline.fit(corpus)\n",
    "tfidf_df = model.transform(corpus)\n",
    "\n",
    "print('TF-IDF pipeline fitted âœ“')\n",
    "tfidf_df.select('doc_id', 'post_count').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Extract top N words per subreddit+quarter â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Spark's TF-IDF gives vectors; we need to map back to actual words\n",
    "# We do this with a UDF that extracts the top-K indices + their scores\n",
    "\n",
    "from pyspark.sql.types import ArrayType, StructType, StructField, StringType, DoubleType, IntegerType\n",
    "from pyspark.ml.linalg import SparseVector, DenseVector\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# Get vocabulary by hashing words and extracting top terms\n",
    "# Since we use HashingTF (not CountVectorizer), we need a different approach:\n",
    "# collect the words + their TF-IDF scores from the raw tokens\n",
    "\n",
    "@F.udf(returnType=ArrayType(StructType([\n",
    "    StructField('word', StringType()),\n",
    "    StructField('tfidf_score', DoubleType())\n",
    "])))\n",
    "def top_words_udf(words, features_vector, top_n=15):\n",
    "    \"\"\"Map top TF-IDF feature indices back to words.\"\"\"\n",
    "    if not words or not features_vector:\n",
    "        return []\n",
    "    # Build wordâ†’hash mapping\n",
    "    from pyspark.ml.feature import HashingTF as HTF\n",
    "    import hashlib\n",
    "\n",
    "    num_features = 10000\n",
    "    word_scores = {}\n",
    "\n",
    "    if hasattr(features_vector, 'indices'):\n",
    "        # SparseVector\n",
    "        idx_to_score = dict(zip(features_vector.indices, features_vector.values))\n",
    "    else:\n",
    "        idx_to_score = {i: v for i, v in enumerate(features_vector) if v > 0}\n",
    "\n",
    "    for word in set(words):\n",
    "        # Replicate Spark's murmur3 hash used by HashingTF\n",
    "        h = abs(hash(word)) % num_features\n",
    "        if h in idx_to_score:\n",
    "            word_scores[word] = idx_to_score[h]\n",
    "\n",
    "    sorted_words = sorted(word_scores.items(), key=lambda x: -x[1])[:top_n]\n",
    "    return [(w, float(s)) for w, s in sorted_words]\n",
    "\n",
    "\n",
    "# Apply the UDF\n",
    "with_top_words = tfidf_df.withColumn(\n",
    "    'top_terms',\n",
    "    top_words_udf(F.col('words'), F.col('features'))\n",
    ")\n",
    "\n",
    "# Explode into rows for easy display\n",
    "exploded = (\n",
    "    with_top_words\n",
    "    .select('subreddit', 'quarter', 'post_count',\n",
    "            F.explode('top_terms').alias('term'))\n",
    "    .select('subreddit', 'quarter', 'post_count',\n",
    "            F.col('term.word').alias('word'),\n",
    "            F.round(F.col('term.tfidf_score'), 4).alias('tfidf_score'))\n",
    "    .withColumn('rank',\n",
    "        F.rank().over(\n",
    "            Window.partitionBy('subreddit', 'quarter')\n",
    "            .orderBy(F.desc('tfidf_score'))\n",
    "        )\n",
    "    )\n",
    "    .filter(F.col('rank') <= 10)\n",
    "    .orderBy('subreddit', 'quarter', 'rank')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:>                 (0 + 2) / 5][Stage 26:>                 (0 + 0) / 2]"
     ]
    }
   ],
   "source": [
    "# â”€â”€ Show topic drift for specific subs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "for sub in ['collapse', 'politics', 'wallstreetbets', 'femaledatingstrategy']:\n",
    "    print(f'\\n=== TOP TERMS: r/{sub} over time ===')\n",
    "    (\n",
    "        exploded\n",
    "        .filter(F.col('subreddit') == sub)\n",
    "        .select('quarter', 'rank', 'word', 'tfidf_score')\n",
    "        .orderBy('quarter', 'rank')\n",
    "    ).show(40, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Save top terms to Gold â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "(\n",
    "    exploded.write\n",
    "    .mode('overwrite')\n",
    "    .parquet('../data/gold/topic_terms')\n",
    ")\n",
    "print('Topic terms saved âœ“')\n",
    "\n",
    "spark.stop()\n",
    "print('Topic drift analysis complete âœ“')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

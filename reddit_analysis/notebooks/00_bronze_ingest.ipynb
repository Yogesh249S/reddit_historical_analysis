{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¥‰ Bronze â€” Ingest Real Pushshift JSONL\n",
    "\n",
    "**Run this first, once.** Reads your 20GB JSONL files, enforces schema, writes partitioned Parquet.\n",
    "\n",
    "Place all your `.jsonl` files in `data/bronze/raw/` before running.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/26 17:12:35 WARN Utils: Your hostname, terminator resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "26/02/26 17:12:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/02/26 17:12:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark ready: 3.5.1\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName('Reddit-Bronze')\n",
    "    .master('local[2]')\n",
    "    .config('spark.driver.memory', '4g')\n",
    "    .config('spark.sql.shuffle.partitions', '8')\n",
    "    .config('spark.sql.adaptive.enabled', 'true')\n",
    "    # Pushshift files are large â€” increase max partition bytes\n",
    "    .config('spark.sql.files.maxPartitionBytes', '256m')\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel('WARN')\n",
    "print('Spark ready:', spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pushshift schema â€” covers all fields we actually need\n",
    "# Any field not in this schema is silently dropped (saves memory)\n",
    "SCHEMA = StructType([\n",
    "    StructField('id',                StringType(),  False),\n",
    "    StructField('subreddit',         StringType(),  False),\n",
    "    StructField('author',            StringType(),  True),\n",
    "    StructField('title',             StringType(),  True),\n",
    "    StructField('selftext',          StringType(),  True),\n",
    "    StructField('score',             IntegerType(), True),\n",
    "    StructField('upvote_ratio',      DoubleType(),  True),\n",
    "    StructField('num_comments',      IntegerType(), True),\n",
    "    StructField('created_utc',       LongType(),    False),\n",
    "    StructField('url',               StringType(),  True),\n",
    "    StructField('domain',            StringType(),  True),\n",
    "    StructField('is_self',           BooleanType(), True),\n",
    "    StructField('over_18',           BooleanType(), True),\n",
    "    StructField('stickied',          BooleanType(), True),\n",
    "    StructField('locked',            BooleanType(), True),\n",
    "    StructField('removed_by_category', StringType(), True),\n",
    "    StructField('link_flair_text',   StringType(),  True),\n",
    "    StructField('total_awards_received', IntegerType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:======================================================>(124 + 1) / 125]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw count: 15339133\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- subreddit: string (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- selftext: string (nullable = true)\n",
      " |-- score: integer (nullable = true)\n",
      " |-- upvote_ratio: double (nullable = true)\n",
      " |-- num_comments: integer (nullable = true)\n",
      " |-- created_utc: long (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- domain: string (nullable = true)\n",
      " |-- is_self: boolean (nullable = true)\n",
      " |-- over_18: boolean (nullable = true)\n",
      " |-- stickied: boolean (nullable = true)\n",
      " |-- locked: boolean (nullable = true)\n",
      " |-- removed_by_category: string (nullable = true)\n",
      " |-- link_flair_text: string (nullable = true)\n",
      " |-- total_awards_received: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "#RAW_PATH    = '../data/bronze/raw/*.jsonl'\n",
    "#BRONZE_PATH = '../data/bronze/parquet'\n",
    "RAW_PATH    = '/mnt/c/Users/gusmc/OneDrive/Desktop/reddit_historical_data/data/bronze/raw/*.jsonl'\n",
    "BRONZE_PATH = '/mnt/c/Users/gusmc/OneDrive/Desktop/reddit_historical_data/data/bronze/parquet'\n",
    "\n",
    "raw = (\n",
    "    spark.read\n",
    "    .schema(SCHEMA)\n",
    "    .option('mode', 'PERMISSIVE')\n",
    "    .json(RAW_PATH)\n",
    ")\n",
    "\n",
    "print('Raw count:', raw.count())\n",
    "raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|           subreddit|  count|\n",
      "+--------------------+-------+\n",
      "|            politics|4036663|\n",
      "|           worldnews|2114916|\n",
      "|        conservative| 567287|\n",
      "|            antiwork| 449106|\n",
      "|      wallstreetbets| 418606|\n",
      "|      trueoffmychest| 404177|\n",
      "|    unpopularopinion| 318253|\n",
      "|         formuladank| 276386|\n",
      "|    soccercirclejerk| 233001|\n",
      "|               aitah| 200916|\n",
      "|        changemyview| 148832|\n",
      "|            collapse|  67455|\n",
      "|       dating_advice|  57739|\n",
      "|femaledatingstrategy|  48006|\n",
      "+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add partition columns + clean up obvious junk\n",
    "bronze = (\n",
    "    raw\n",
    "    .filter(F.col('id').isNotNull() & F.col('subreddit').isNotNull())\n",
    "    # Remove deleted/removed posts (text is '[deleted]' or '[removed]')\n",
    "    .filter(~F.lower(F.col('selftext')).isin('[deleted]', '[removed]'))\n",
    "    .withColumn('created_ts',  F.to_timestamp(F.from_unixtime('created_utc')))\n",
    "    .withColumn('year',        F.year('created_ts').cast('string'))\n",
    "    .withColumn('month',       F.lpad(F.month('created_ts').cast('string'), 2, '0'))\n",
    "    .withColumn('subreddit',   F.lower(F.trim('subreddit')))\n",
    "    .withColumn('author',      F.lower(F.trim('author')))\n",
    "    .withColumn('ingested_at', F.current_timestamp())\n",
    "    # Combine title + selftext into one searchable text field\n",
    "    .withColumn('full_text',\n",
    "        F.concat_ws(' ',\n",
    "            F.coalesce('title', F.lit('')),\n",
    "            F.coalesce('selftext', F.lit(''))\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Show subreddit distribution\n",
    "bronze.groupBy('subreddit').count().orderBy(F.desc('count')).show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bronze done âœ“\n"
     ]
    }
   ],
   "source": [
    "# Write Bronze â€” partition by subreddit + year\n",
    "# For 20GB, this will take 10â€“20 minutes on local\n",
    "(\n",
    "    bronze.write\n",
    "    .mode('overwrite')\n",
    "    .partitionBy('subreddit', 'year')\n",
    "    .parquet(BRONZE_PATH)\n",
    ")\n",
    "print('Bronze done âœ“')\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
